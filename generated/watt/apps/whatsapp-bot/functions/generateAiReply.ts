// @generated by flusk-lang logic compiler
import type { FastifyInstance, FastifyRequest, FastifyReply } from 'fastify';

interface GenerateAiReplyInput {
  botId: string;
  conversationId: string;
  userMessage: string;
}

export default async function generateAiReply(
  app: FastifyInstance,
  request: FastifyRequest,
  reply: FastifyReply,
  input: GenerateAiReplyInput,
) {
  const bot = await app.platformatic.entities.bot.find({ where: { id: input.bot_id }, limit: 1 }).then(r => r[0] ?? null);
  if (!(bot)) {
    return reply.code(404).send({ error: 'Bot not found' });
  }
  if (!(bot.active)) {
    return reply.code(400).send({ error: 'Bot is not active' });
  }
  const history = await app.platformatic.entities.message.find({ where: { conversation_id: input.conversation_id } });
  const messages = [{ role: 'system', content: bot.system_prompt }];
  const results = [];
  for (const msg of history) {
    const messages = messages.concat([{ role: (msg.direction == 'in\' ? \'user\' : \'assistant'), content: msg.body }]);
  }
  const messages = messages.concat([{ role: 'user', content: input.user_message }]);
  const start = Date.now();
  const result = http.post('https://api.openai.com/v1/chat/completions', { model: bot.model, messages, temperature: bot.temperature, max_tokens: bot.max_tokens });
  const latency_ms = Date.now() - start;
  const reply = result.choices[0].message.content;
  const tokens_used = (result.usage.total_tokens ?? 0);
  const cost = tokens_used * 0.00001;
  return reply.send({ reply, tokens_used, cost, latency_ms });
}
